\section{Mutual information}
We now consider the case where we have a set of images. This can be
R,G,B color band images, observations of the same area but 
at different times, etc.
We are interested in having an entropy measurement 
of such data sets, knowing that the correlation between two images 
can be relatively high. A 3-dimensional wavelet transform is not 
appropriate because the third dimension is generally completely different
from the two first. The standard case is the data set where we have 
2-dimensional spatial information versus a frequency band.

\subsection{Principal component analysis}
Principal Component Analysis (PCA), also often referred as eigenvector,
Hotelling, 
or Karhunen-Lo\`eve transform 
\cite{ima:karhunen47,ima:loeve48,ima:hotelling33},
allows us to transform  discrete signals into a sequence of uncorrelated 
coefficients. Considering a population $D(1..M,1..N)$ 
of $M$ signals or images of dimension $N$,
the PCA method consists of expressing the dataset $D$ by:
\begin{eqnarray}
D = U \Lambda^{\frac{1}{2}} V^t
\end{eqnarray}
and
\begin{eqnarray}
DD^t & = & U \Lambda U^t \\
D^t D & = & V \Lambda V^t
\end{eqnarray}
where $\Lambda$ is the diagonal matrix of eigenvalues of the covariance 
matrix $C = DD^t$, the columns of $U$ are the eigenvectors of $C$, and 
the columns of $U$ are the eigenvector of $D^t D$. 

For a signal $d(1..N)$ we have,
\begin{eqnarray}
d = \sum_{i=1}^{M} \sqrt{\lambda_i} u_i v_i^t
\end{eqnarray}
where $\lambda_i$ are the eigenvalues of the covariance matrix. The $v_i$
vectors can also be calculated by \cite{ima:bijaoui79}:
\begin{eqnarray}
 V(i,j) = v_i(j) = \frac{1}{\sqrt{\lambda_i}} \sum_k D(i,k) u_i(j)
\end{eqnarray}

In practice, we construct the matrix $A$ whose rows are formed 
from the eigenvectors
of $C$ \cite{ima:gonzalez93}, ordered following the 
monotonic decreasing order of eigenvalues. A vector $x(1..M)$ can then be
transformed by:
\begin{eqnarray}
y = \Lambda^{-\frac{1}{2}}A(x-m_x)
\end{eqnarray}
where $m_x$ is the mean value of $x$.
Because the rows of $A$ are orthonormal vectors, $A^{-1} = A^t$, and
any vector $x$ can be recovered from its corresponding $y$ by:
\begin{eqnarray}
x = \Lambda^{\frac{1}{2}}A^t y + m_x
\end{eqnarray}

The $\Lambda$ matrix multiplication can be seen as a normalization.
Building $A$ from the correlation matrix instead of the covariance matrix
leads to another kind of normalization, 
and the $\Lambda$ matrix can be suppressed 
($y = A(x-m_x)$ and $x = A^t y + m_x$). Then the norm of $y$ will be equal to the
norm of $x$.  

\subsection{The WT-PCA transform}

We consider now that we have $M$ observations of the same view, but
at different wavelengths (or at different epochs, etc.), and denote 
as $D_k$ one 
observation, and $W(k, 1..P)$ its wavelet transform, $P$ being the
number of scales (or bands) of the wavelet transform. Then for a given
frequency band $j$, we can compare the information content for all observations.
As a strong correlation may exist between the same frequency band of two 
different observations, we can appy a principal component analysis for
the specific scale $j$, and repeat the same information for all $j$. Then,
for each scale $j$, we build a correlation matrix $C_j$ (and also $A_j$), 
and the wavelet coefficients are transformed into their principal 
components. As the obtained coefficients are obtained by applying
successively a wavelet transform and a principal component analysis,
we will term these two transformations a WT-PCA transform,
and the values obtained will be called WT-PCA coefficients.

The advantages of the WT-PCA transform are:
\begin{itemize}
\item We have separated the information not only spatially as if we had
used only a wavelet transform, but also on the wavelength axis as if
had used directly a PCA on the images.
\item We keep the possibility open to estimate the noise level of the  
WT-PCA coefficients in  the same rigorous way as for a simple wavelet 
transform.
\item We have an exact reconstruction.
\end{itemize}
 
\subsection{Entropy from the WT-PCA transform}

The entropy relative to a set of observations $D(1..M)$ can be written by:
\begin{eqnarray}
H(D) = \sum_{j=1}^{l} \sum_{e=1}^{M}  \sum_{k=1}^{N_j} h(c_{j,k,e})
\end{eqnarray}
where $l$ is the number of scales used in the wavelet transform
decomposition, $M$ the number of observations, $k$ a pixel position,
 $c$ a WT-PCA coefficients, and $e$ denotes the eigenvector number.

The last scale of the wavelet transform is not used, as previously, so
this entropy measurement is background independent, which is really
important because the background can vary from one wavelength to another.


