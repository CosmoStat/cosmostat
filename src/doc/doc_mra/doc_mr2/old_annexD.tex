\section*{Appendix D: Regularization parameter derivative}
\addcontentsline{toc}{section}{Appendix D: Regularization parameter derivative}
Using the gradient method, the solution $x_{n+1}$ at iteration $n+1$ is found 
from the solution $x_n$ at iteration $n$ by
\begin{eqnarray}
x_{n+1} = x - d_s -  \alpha d_n
\end{eqnarray}
with $d_s = \frac{\partial h_s(y-x)}{\partial x}$ and $d_n = \frac{\partial h_n(x)}{\partial x}$.

and the final solution $x$ should verify
\begin{eqnarray}
\frac{1}{N} \sum_{pix} (\frac{y-x}{\sigma})^2 = 1
\end{eqnarray}
where $N$ is the number of pixels. If $\alpha$ is too high, the solution will
be oversmoothed and the residual standard deviation will be greater than the 
noise, when a too small $\alpha$ will generate a noisy solution, and the 
residual standard deviation will be smaller than the noise.
Noting $R_{\sigma}  = \frac{1}{N} \sum_{pix} (\frac{y-x_{n+1}}{\sigma})^2 - \sigma$,
the difference between the standard deviation of the residual and the noise,
the optimal $\alpha$ at iteration $n$ is found by minimizing the functional:
\begin{eqnarray}
K(\alpha) & = & \mid  R_{\sigma} \mid \\
          & = & \mid \frac{1}{\sigma^2}\sum_{pix} (y^2  +  x_{n+1}^2 - 2 x_{n+1} y - N) \mid
\end{eqnarray}
and 
\begin{eqnarray}
\frac{\partial K}{\partial \alpha} = \frac{2}{N\sigma^2} \sum_{pix}  
 (\frac{\partial x_{n+1}}{\partial \alpha} x_{n+1} 
   - 2y \frac{\partial x_{n+1}}{\partial \alpha})
\end{eqnarray}
when $R_{\sigma} > 0$, and the opposite otherwise.
As $\frac{\partial x_{n+1}}{\partial \alpha} = - d_n$, we have
\begin{eqnarray}
\frac{\partial K}{\partial \alpha} & = & \frac{2}{N\sigma^2} \sum_{pix} (y d_n - d_n x_{n+1})) \\
                                   & = & \frac{2d_n }{N\sigma^2}\sum_{pix}  (y - x_{n+1})
\end{eqnarray}
Then the $\alpha$ parameter is recalculated at each iteration by:
\begin{eqnarray}
\alpha_{n+1} = \alpha_n - sgn(R_{\sigma}) \frac{2d_n }{N\sigma^2} \sum_{pix} (y - x_{n+1})
\end{eqnarray}

  
