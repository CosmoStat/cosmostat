\chapter{ISOCAM faint source detection}
\section{Introduction}

The simple calibration described above is successful when applied 
to bright objects (down to a 
few percent of the background level) but is inefficient when applied 
to faint source detection (below 1~\% of the background) with ISOCAM. 
At first order, this can be improved by modeling the flat-field, 
instead of using a library flat-field. The position of the 
lens of ISOCAM varies slightly between settings, 
and the optical flat-field 
varies as a function of the lens position by 2 to 20~\% from the 
center to the border of the array.  In the case of empty fields (and 
more generally when most of the map covers an empty field), a simple 
median of the cube of data gives a very good flat-field, which allows 
us to reach a detection level of a few ten percent of the background 
level \cite{starck:sta99_1}. 

However, at second order, one encounters the main difficulty in
dealing with ISOCAM faint source detection: the combination of the
cosmic ray impacts (glitches) and the transient behavior of the
detectors. For glitches producing single fast increases and
decreases of the signal,  a simple median filtering produces a fairly 
good deglitching. The ISOCAM glitch rate is one per second, 
and each glitch on average  has an impact on   
eight pixels \cite{starck:claret99}. However, 5 to 20~\% of the
total number of readouts, depending on the integration time and
the strength of the selection criterion, are affected by memory effects,
which can produce false detections. Consequently, the main limitation
here is not the detection limit of the instrument, which is quite low, 
but the false detections, whose number increases with the sensitivity. \\
Three types of glitches can be isolated, those creating:
\begin{enumerate}
\item a positive strong and short feature (lasting one readout only).  
\item a positive tail ({\em fader}, lasting a few readouts).
\item a negative tail ({\em dipper}, lasting a several tens of readouts).  
\end{enumerate}

\begin{figure}[htb]
\centerline{
\hbox{
\psfig{figure=fig_3nglitch.ps,bbllx=2.cm,bblly=12.5cm,bburx=20cm,bbury=25.5cm,width=16cm,height=12cm}
}}
\caption{These three plots show a single detector response in Analog to
Digital Units (ADU) as a function of time, expressed here in number of
readouts,  where each readout corresponds to 2s. Three types glitches 
due to cosmic rays can be found here:
(a) top: the most common glitches, lasting only one readout. (b)
middle: a ``fader'', occuring around readout 160. This glitch presents a slowly
decreasing tail. It has been truncated above 100 ADUs, but its original
intensity is 2558 ADUs.  (c) bottom: a ``dipper'', beginning around readout
240. This glitch is followed by a memory effect lasting about 100 readouts.
In these observations, the camera draws a mosaic on the sky (raster). Hence as
long as an object, such as a star or a galaxy, is in the line of sight of
a given pixel, the measured signal increases.  A faint galaxy is visible 
on the bottom plot (c) around readout 120, lasting only one position
in the raster.}

\label{fig_glitch3}
\end{figure}



Figure~\ref{fig_glitch3} is 
a plot in camera units (ADU, for Analog to Digital Units) measured 
by a single pixel as a function of the number of readouts, i.e.  time, 
which shows these three types of glitches: (a) three sharp "1" type, (b)
 a "fader" at readout 80 and lasting 20 
readouts, (c) a "dipper" at readout 230 lasting 150 readouts.  

The two first pixels are taken from a four by four raster observation of the
Lockman hole, with a 
pixel field of view of 6 arc second, an individual integration time 
of 2.1 second,
the LW3 filters (15$\mu$m), a gain of 2, and 56 readout for the first raster
position and 27 readout for the others 
(TDT=03000102: revolution 30, observation 102).

The last pixel is from another observation, with the same parameters except for
the number of readouts per raster position, which is equal to 22 instead of 27.
(TDT=02600404).
 
Finally, the signal measured by a single pixel as a function of time
is the combination of memory effects, cosmic ray impacts and real
sources: memory effects begin with the first readouts, since the
detector faces a flux variation from an offset position to the target
position (stabilization), then appear with long-lasting glitches and
following the detection of real sources. Clearly one needs to
 separate all these
components of the signal in each pixel before building a final
raster map, and to keep the information of the associated noise before
applying a source detection algorithm.  

\section{Pattern REcognition Technique for ISOCAM}
\begin{figure}[htb]
\centerline{
\vbox{
\psfig{figure=fig_iso_plot6.ps,bbllx=2.cm,bblly=13cm,bburx=19.5cm,bbury=25.5cm,width=17cm,height=12.5cm}}}
\caption{Decomposition of the signal into its main components: 
(a) short glitch, (b) trough of a dipper, (c) bright source, (d)
baseline, (e) noise plus faint sources. The simple sum of the fives
components is exactly equal to the original data (see figure 2). The
calibrated background free data are obtained by addition of signals
(c) and (e). Figure (c) shows the reconstruction of a source
approximated as a Gaussian, but sources are kept in the signal and
their shape differ from one source to the other}
\label{fig_decomp}
\end{figure}


The idea developed here is to use the multi-scale vision modeling for
a decomposition of a signal into its main components. In practice, a
simple object reconstruction from the detected structure in the
wavelet space, as proposed in Bijaoui \& Ru\'e \cite*{ima:bijaoui95},
would produce poor results because of the strong confusion between the
numerous objects that can be found in the data. Moreover, wavelet
transforms present a drawback: the wings of the wavelet function are
negative (so that the integral of the function is zero) which implies that
when a positive signal falls onto one wing of the wavelet function it
produces a negative signal in the wavelet transform.  The quality of
the object reconstruction is good only when additional constraints are
introduced, such as positivity constraint for positive objects, and negativity
constraint for negative objects. An object is defined as positive
(or negative) when the wavelet coefficient of the object,
which has the maximum absolute value, is positive (or negative).

The problem of confusion between numerous objects can be solved when
including a selection criterion in the detection of these
objects. Using the knowledge we have about the objects, in this case, glitches,
the problem of unknown object reconstruction is reduced to a pattern
recognition problem, where the pattern is the glitch itself. We only
search for objects which satisfy a given set of conditions in the
Multi-Scale Vision Model (MVM). For example, finding glitches of the
first type is equivalent to finding objects which are positive, strong,
and with a duration shorter than those of the sources. The method that
we use for the decomposition of the signal of a given ISOCAM pixel,
$D(t_0..t_n)$, is summarized below:
 
\begin{enumerate}
\item detection of the glitches of the first type in wavelet
space: the corresponding signal, $C_1(t_0..t_n)$, is then subtracted
from the initial data, D: $D_1 = D - C_1$. This is the first deglitching step.
\item detection of the negative components due to dippers: the
multi-scale vision model is applied to $D_1$, hence negative objects
are detected and the reconstructed signal, $C_2(t_0..t_n)$, is  
subtracted to the output of the previous step: $D_2 = D_1 -
C_2$. This is the second deglitching step where throughs following
glitches are corrected.
\item detection of the positive components due to faders and dippers:
this step must be done carefully, since sources also produce positive
components in the signal. Only positive objects lasting much longer
or much less than the time spent on a given position  on
the sky are automatically
considered as glitches. The output signal, $C_3(t_0..t_n)$, is then
subtracted again from the previous signal: $D_3 = D_2 - C_3$.
\item detection of a very strong positive signal on scales where
sources are expected. This step is done in preparation for the baseline
subtraction; the final source detection is not done at this stage.
The multiscale vision model is applied to $D_3$ and
strong positive objects with a correct temporal size are
reconstructed: we obtain $C_4(t_0..t_n)$, and we calculate $D_4 = D_3 -
C_4$.
\item baseline subtraction: the signal $D_4$ contains only noise and 
temporally undetectable faint sources. 
The baseline is easily obtained by convolving $D_4$ by
a low frequency pass band filter. 
We obtain  $C_5(t_0..t_n)$.
\item The residual noise is obtained by $C_6 = D_4 - C_5$; its mean value is 
zero.
\end{enumerate}

Finally, the set $(C_1,C_2,C_3,C_4,C_5,C_6)$, represents the
decomposition of the signal into its main components. Note also
that the input signal $D$ is equal to the sum of all components:
\begin{eqnarray}
D = \sum_{i=1}^{6} C_i
\end{eqnarray}
A deglitched signal is obtained by:
\begin{eqnarray}
D_g = D - C_1 - C_2 - C_3
\end{eqnarray}
For   faint source detection, we use the signal $D_b = C_4 + C_6$,
which is background, dark, and glitch free. The background has been
subtracted, and glitches with their long duration effects have been
suppressed. Applying the pattern recognition method to all detector
pixels, we obtain a cube $D_b(x,y,t)$. All other component are kept in
the cubes $C_i$.  The baseline suppression presents several
advantages: first, the final raster map is dark-corrected
without the need of a library dark, since we end up with a mean zero
level for each pixel.  This is particularly important when the library
dark is not good enough, and induces visual artifacts    
\cite{starck:sta99_1}. Second, the flat-field accuracy only affects 
the photometry
of the sources but not the background level, which is extracted in the
baseline. Thus, its influence in the final calibration is decreased.


\subsection*{Example}
\begin{figure}[htb]
\centerline{ \vbox{
\psfig{figure=fig_iso_plot7.ps,bbllx=2.5cm,bblly=13cm,bburx=19.5cm,bbury=25.5cm,width=17cm,height=12.5cm}}}
\caption{These two plots show the signal of a single pixel as a function of time 
before calibration (top, flux in ADU) and after calibration (bottom, flux in 
ADU/gain/second). The trough following the second glitch (dipper) has 
disappeared and the remaining signal contains only Gaussian noise (photon noise 
+ readout noise) plus sources (one relatively bright source is located at 
readout 120, fainter sources will only appear after co-addition of all pixels 
having seen the same sky position in the final map).}
\label{fig_calib}
\end{figure}

Figure~\ref{fig_calib} (bottom) presents the result obtained with this
method. The decomposition of the original signal
(figure~\ref{fig_calib} top) into its main components is shown on
Figure~\ref{fig_decomp}: (a), (b), and (d) are features subtracted
from the original signal (short glitches, dipper, and
baseline, respectively), which present no direct interest for faint source
detection, and only (c) and (e) (bright sources and noise plus
faint sources, respectively) are kept for building the final image. 
The noise must also be kept because faint sources are often detectable 
only after co-addition of the data.  The
simple sum of the five components is exactly equal to the original
data (see figure~\ref{fig_calib} top). The calibrated background free
data (see figure~\ref{fig_calib} bottom) are then obtained by addition
of (c) and (e).

In theory, a source should have a top-hat profile, but its shape is
modified by transient effects, which are very important for faint sources.
The use of a wavelet function, which has a smooth shape, 
enhances even more the Gaussian shape of the reconstructed source.

\clearpage
\newpage

\section{The Multiscale Median Transform}
\begin{figure}[htb] 
\centerline{
\hbox{
\psfig{figure=fig_wt_glitch.ps,bbllx=3.3cm,bblly=14.7cm,bburx=11cm,bbury=22.4cm,height=8cm,width=8cm,clip=}
\psfig{figure=fig_mmt_glitch.ps,bbllx=3.3cm,bblly=14.7cm,bburx=11cm,bbury=22.4cm,height=8cm,width=8cm,clip=}
}}
\label{fig_cmp}
\caption{Comparison between   the ``\`a trous'' wavelet transform (left)
and the multresolution median transform (right) of the signal
of Figure 1 (c). Resolution scale is represented versus the time.
We note that the separation between the source and the glitch is
improved using the MMT.}
\end{figure}


\begin{figure}[htb]
\centerline{
\vbox{
\psfig{figure=fig_glitch_median.ps,bbllx=2.cm,bblly=13cm,bburx=19.5cm,bbury=25.5cm,width=17cm,height=12.5cm}}}
\caption{Decomposition of the signal into its main components using
the MMT: 
(a) source, (b)  dipper, (c)  baseline,
 (d) Sum of the glitches and the baseline (i.e. non interesting data),
and (e) original signal minus (d).}
\label{fig_glitch_mmt}
\end{figure}


The presented method produces good results but requires a long computation
time.  A similar but faster method, producing results of
equivalent quality and
avoiding the delicate problem of the negative wings of wavelet
functions, is to use the Multi-Resolution Median Transform (MMT)
\cite{starck:book98} instead of the wavelet transform.  No confusion
between positive and negative objects is possible because this
multi-resolution transform does not present the ringing drawback.  The
MMT has been proposed for data compression \cite{starck:sta96_2}, and
it has also recently been used for ISOCAM short glitch suppression
\cite{starck:sta99_1}.

The MMT algorithm is relatively simple. Let $med(S,n)$ be the median
transform of a one-dimensional signal S within a window of dimension
$n$. If we define, for $N_s$ resolution scales, the coefficients:
\begin{eqnarray}  
c_i & = & \left\{ \begin{array}{ll}
	S  &  \mbox{   if } i=1 \\
        med(S,2^{i-1}+1) & \mbox{   if } i=2,N_s \\ 
	\end{array}
	\right.
\end{eqnarray} 
\begin{eqnarray}  
\begin{array}{lc}
w_i=c_{i-1}-c_i      & \mbox{ for } i=2,N_s
\end{array}
\end{eqnarray} 

we can expand the original signal similarly to the ``\`a trous algorithm'':
\begin{equation}
S = c_p + \sum_{i=2,N_s}  w_i,
\end{equation}
where $c_p$ is the residual signal.

Applying the multi-scale vision model (MVM) with the MMT, the object
reconstruction is straightforward, because no iteration is needed to
get a good quality reconstruction.

Figure~\ref{fig_cmp} shows the 
comparison between the ``\`a trous'' wavelet transform (on the left)
and the MMT (on the right) of the input signal in Figure 1 (c). 
In these diagrams, the scale is represented as a function of the time.
We can see that with the MMT, it is easier to distinguish  
sources from glitches.
Figure~\ref{fig_glitch_mmt} shows the result of MVM using the MMT instead
of the ``\`a trous'' algorithm. From (a) to (e), we see   
the reconstructed source, the dipper, the baseline,
the sum of the glitches and the baseline (i.e. non-interesting data),
and the original signal from which the signal (d) has been subtracted.

\section{Conclusion}
Once the calibration is done, the raster map
can normally  be created, with flat field correction, and all data
co-added. The associated rms map can now be used for the detection,
which was impossible before due to the strong effect of residual
glitches. Since the background has been removed, simple source detection
can be done just by comparing the flux in the raster map to the rms
map.

\clearpage
